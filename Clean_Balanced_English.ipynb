{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "feb72d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca9dbd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accepted': 77500.0, 'pending': 70000, 'blocked': 70000}\n"
     ]
    }
   ],
   "source": [
    "# ──────────────── CONFIGURATION ────────────────\n",
    "\n",
    "x = 5000\n",
    "\n",
    "\n",
    "# 1) Desired number of accepted/pending/blocked examples for each label:\n",
    "DESIRED_COUNTS = {\n",
    "    \"Bullying\":             {\"accepted\": x, \"pending\": x, \"blocked\": x},\n",
    "    \"Fighting\":             {\"accepted\": x, \"pending\": x, \"blocked\": x},\n",
    "    \"Sexting\":              {\"accepted\": x, \"pending\": x, \"blocked\": x},\n",
    "    \"Vulgar\":               {\"accepted\": x, \"pending\": x, \"blocked\": x},\n",
    "    \"Drugs\":                {\"accepted\": x, \"pending\": x, \"blocked\": x},\n",
    "    \"InGame\":               {\"accepted\": x, \"pending\": x, \"blocked\": x},\n",
    "    \"Alarm\":                {\"accepted\": x, \"pending\": x, \"blocked\": x},\n",
    "    \"Fraud\":                {\"accepted\": x, \"pending\": x, \"blocked\": x},\n",
    "    \"Racist\":               {\"accepted\": x, \"pending\": x, \"blocked\": x},\n",
    "    \"Religion\":             {\"accepted\": x, \"pending\": x, \"blocked\": x},\n",
    "    \"Junk\":                 {\"accepted\": x, \"pending\": x, \"blocked\": x},\n",
    "    \"Website\":              {\"accepted\": x, \"pending\": x, \"blocked\": x},\n",
    "    \"Grooming\":             {\"accepted\": x, \"pending\": x, \"blocked\": x},\n",
    "    #\"PublicThreats\":        {\"accepted\": x, \"pending\": x, \"blocked\": x},\n",
    "  #  \"RealName\":             {\"accepted\": x, \"pending\": x, \"blocked\": x},\n",
    "    #\"ExtremistRecruitment\": {\"accepted\": x, \"pending\": x, \"blocked\": x},\n",
    "   # \"Subversive\":           {\"accepted\": x, \"pending\": x, \"blocked\": x},\n",
    "   # \"Sentiment\":            {\"accepted\": x, \"pending\": x, \"blocked\": x},\n",
    "    \"Politics\":             {\"accepted\": x, \"pending\": x, \"blocked\": x},\n",
    "    # \"Nothing Wrong\" only uses 'accepted':\n",
    "    \"Nothing Wrong\":        {\"accepted\": 1.5*x, \"pending\": 0, \"blocked\": 0},\n",
    "}\n",
    "\n",
    "# 2) Compute global totals dynamically from DESIRED_COUNTS\n",
    "DESIRED_OVERALL = {\"accepted\": 0, \"pending\": 0, \"blocked\": 0}\n",
    "for label_counts in DESIRED_COUNTS.values():\n",
    "    for decision in DESIRED_OVERALL:\n",
    "        DESIRED_OVERALL[decision] += label_counts.get(decision, 0)\n",
    "\n",
    "# ──────────────── END CONFIGURATION ────────────────\n",
    "\n",
    "# Running counts that mirror DESIRED_COUNTS but start at 0\n",
    "counts = {\n",
    "    label: {\"accepted\": 0, \"pending\": 0, \"blocked\": 0}\n",
    "    for label in DESIRED_COUNTS\n",
    "}\n",
    "\n",
    "# Global totals to track progress\n",
    "global_counts = {\"accepted\": 0, \"pending\": 0, \"blocked\": 0}\n",
    "\n",
    "# To track duplicate texts\n",
    "seen_texts = set()\n",
    "\n",
    "print(DESIRED_OVERALL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1873f9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────── STEP 1: Helper to tell us if we should stop ────────────────\n",
    "def stop_criteria_met():\n",
    "    \"\"\"\n",
    "    Return True if EITHER\n",
    "      a) ALL per-label targets are reached (for every label, each decision bucket is >= desired),\n",
    "      OR\n",
    "      b) ALL global targets are reached (accepted >= DESIRED_OVERALL['accepted'], etc.).\n",
    "    \"\"\"\n",
    "    # a) Check per-label:\n",
    "    all_labels_full = True\n",
    "    for label, desired in DESIRED_COUNTS.items():\n",
    "        for dec in [\"accepted\", \"pending\", \"blocked\"]:\n",
    "            # If DESIRED_COUNTS[label][dec] is zero, we consider it \"already satisfied\"\n",
    "            # as long as our counts[label][dec] >= 0 (which is always true).\n",
    "            if desired[dec] > 0 and counts[label][dec] < desired[dec]:\n",
    "                all_labels_full = False\n",
    "                break\n",
    "        if not all_labels_full:\n",
    "            break\n",
    "\n",
    "    # b) Check global:\n",
    "    all_global_full = True\n",
    "    for dec in [\"accepted\", \"pending\", \"blocked\"]:\n",
    "        if global_counts[dec] < DESIRED_OVERALL[dec]:\n",
    "            all_global_full = False\n",
    "            break\n",
    "\n",
    "    return all_labels_full or all_global_full\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82b2479f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────── STEP 2: Process one CSV file ────────────────\n",
    "def process_csv_file(path):\n",
    "    \"\"\"\n",
    "    Reads `path` into a DataFrame, filters for English rows without {{…}} tokens,\n",
    "    then for each row decides:\n",
    "      - if all label‐columns are zero → treat as \"Nothing Wrong\"\n",
    "      - otherwise find the first non-zero label, map its status → decision,\n",
    "        check if that label‐decision bucket is still under its DESIRED_COUNTS,\n",
    "        and if so, add it. Also skip duplicates on `text`.\n",
    "    Returns a list of row‐dicts to append, and updates the global counts.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    # Keep only English:\n",
    "    df = df[df[\"user_primary_language\"] == \"en\"]\n",
    "    # Filter out any row whose text contains {{…}}:\n",
    "    df = df[~df[\"text\"].astype(str).str.contains(r\"\\{\\{.*?\\}\\}\", regex=True)]\n",
    "\n",
    "    # Only keep these label columns (plus text & language):\n",
    "    label_cols = [\n",
    "        \"Bullying\", \"Fighting\", \"Sexting\", \"Vulgar\", \"Drugs\", \"InGame\", \"Alarm\", \"Fraud\",\n",
    "        \"Racist\", \"Religion\", \"Junk\", \"Website\", \"Grooming\", \"Politics\"\n",
    "    ]\n",
    "\n",
    "#\"PublicThreats\"\"Sentiment\"\"ExtremistRecruitment\"\n",
    "\n",
    "    df[label_cols] = df[label_cols].fillna(0)\n",
    "\n",
    "    rows_to_add = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        if stop_criteria_met():\n",
    "            # If either per-label OR global targets are satisfied, stop immediately.\n",
    "            break\n",
    "\n",
    "        text = row[\"text\"]\n",
    "        # 1) Dedupe on text:\n",
    "        if text in seen_texts:\n",
    "            continue\n",
    "\n",
    "        # 2) Check if all label_cols are zero → \"Nothing Wrong\"\n",
    "        is_all_zero = True\n",
    "        for lc in label_cols:\n",
    "            if int(row[lc]) != 0:\n",
    "                is_all_zero = False\n",
    "                break\n",
    "\n",
    "        # If \"Nothing Wrong\":\n",
    "        if is_all_zero:\n",
    "            label = \"Nothing Wrong\"\n",
    "            decision = \"accepted\"  # always accepted for \"Nothing Wrong\"\n",
    "            # Check if we still need more of this:\n",
    "            if counts[label][decision] < DESIRED_COUNTS[label][decision]:\n",
    "                # Add one\n",
    "                counts[label][decision] += 1\n",
    "                global_counts[decision] += 1\n",
    "                seen_texts.add(text)\n",
    "                rows_to_add.append({\n",
    "                    \"Text\": text,\n",
    "                    \"Label\": label,\n",
    "                    \"Status\": 0,\n",
    "                    \"Decision\": decision\n",
    "                })\n",
    "            # either way, skip any further labels \n",
    "            continue\n",
    "\n",
    "        # Otherwise, find the first non-zero label in label_cols:\n",
    "        for lc in label_cols:\n",
    "            status = int(row[lc])\n",
    "            if status == 0:\n",
    "                continue\n",
    "\n",
    "            # Map status → decision:\n",
    "            #   0,1,2  → accepted\n",
    "            #   3,4    → pending\n",
    "            #   5,6,7  → blocked\n",
    "            if status in [0, 1, 2]:\n",
    "                decision = \"accepted\"\n",
    "            elif status in [3, 4]:  \n",
    "                decision = \"pending\"\n",
    "            elif status in [5, 6, 7]:\n",
    "                decision = \"blocked\"\n",
    "            else:\n",
    "                # any other status code we skip:\n",
    "                continue\n",
    "\n",
    "            label = lc  # e.g. \"Bullying\", \"Fighting\", etc.\n",
    "            # If we still need more of this label/decision:\n",
    "            if counts[label][decision] < DESIRED_COUNTS[label][decision]:\n",
    "                counts[label][decision] += 1\n",
    "                global_counts[decision] += 1\n",
    "                seen_texts.add(text)\n",
    "                rows_to_add.append({\n",
    "                    \"Text\": text,\n",
    "                    \"Label\": label,\n",
    "                    \"Status\": status,\n",
    "                    \"Decision\": decision\n",
    "                })\n",
    "            # Whether we added or not, stop checking other labels for this text:\n",
    "            break\n",
    "\n",
    "    return rows_to_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e11dbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────── STEP 3: Summary‐report generator ────────────────\n",
    "def generate_summary_report(all_rows_df):\n",
    "    \"\"\"\n",
    "    all_rows_df has columns: Text, Label, Status, Decision.\n",
    "    We now show:\n",
    "      1) For each label: #accepted, #pending, #blocked\n",
    "      2) Overall accepted/pending/blocked totals\n",
    "    (Total column has been removed as requested)\n",
    "    \"\"\"\n",
    "    summary_data = []\n",
    "\n",
    "    # A) Overall decision summary:\n",
    "    overall = all_rows_df[\"Decision\"].value_counts().to_dict()\n",
    "    summary_data.append([\"Overall accepted\", overall.get(\"accepted\", 0)])\n",
    "    summary_data.append([\"Overall pending\", overall.get(\"pending\", 0)])\n",
    "    summary_data.append([\"Overall blocked\", overall.get(\"blocked\", 0)])\n",
    "    summary_data.append([])  # Empty row for spacing\n",
    "\n",
    "    # B) Per-label breakdown (including \"Nothing Wrong\"):\n",
    "    summary_data.append([\"Label\", \"Accepted\", \"Pending\", \"Blocked\"])  # Removed \"Total\"\n",
    "    labels = list(DESIRED_COUNTS.keys())\n",
    "    \n",
    "    for label in labels:\n",
    "        a = all_rows_df[(all_rows_df[\"Label\"] == label) & (all_rows_df[\"Decision\"] == \"accepted\")].shape[0]\n",
    "        p = all_rows_df[(all_rows_df[\"Label\"] == label) & (all_rows_df[\"Decision\"] == \"pending\")].shape[0]\n",
    "        b = all_rows_df[(all_rows_df[\"Label\"] == label) & (all_rows_df[\"Decision\"] == \"blocked\")].shape[0]\n",
    "        summary_data.append([label, a, p, b])  # Removed total calculation\n",
    "\n",
    "    # Save to CSV:\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_df.to_csv(\"summary_report_csv.csv\", index=False, header=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # Also print to console:\n",
    "    print(\"\\n📊 Summary Report (Totals Removed):\\n\")\n",
    "    print(\"Overall accepted :\", overall.get(\"accepted\", 0))\n",
    "    print(\"Overall pending  :\", overall.get(\"pending\", 0))\n",
    "    print(\"Overall blocked  :\", overall.get(\"blocked\", 0))\n",
    "    print()\n",
    "    print(f\"{'Label':25s}{'Acc':>8s}{'Pen':>8s}{'Blk':>8s}\")  # Removed 'Total' from header\n",
    "    for row in summary_data[5:]:  # skip first five rows (overall counts and empty row and header)\n",
    "        label, a, p, b = row  # Removed tot from unpacking\n",
    "        print(f\"{label:25s}{a:8d}{p:8d}{b:8d}\")  # Removed tot from print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20317218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────── STEP 4: Process entire folder ────────────────\n",
    "def file_clean_summary_for_folder(folder_path):\n",
    "    all_rows = []\n",
    "\n",
    "    for fname in os.listdir(folder_path):\n",
    "        if not fname.lower().endswith(\".csv\"):\n",
    "            continue\n",
    "        fullpath = os.path.join(folder_path, fname)\n",
    "        print(f\"🔄 Processing file: {fname}…\")\n",
    "\n",
    "        new_rows = process_csv_file(fullpath)\n",
    "        all_rows.extend(new_rows)\n",
    "\n",
    "        if stop_criteria_met():\n",
    "            print(\"✅ Stop criteria met → ending early.\")\n",
    "            break\n",
    "\n",
    "    # Convert to DataFrame (without the TotalForThatLabel column)\n",
    "    final_df = pd.DataFrame(all_rows, columns=[\"Text\", \"Label\", \"Status\", \"Decision\"])\n",
    "    \n",
    "    # Save the final CSV:\n",
    "    final_df.to_csv(\"final_csv.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    print(\"✅ Saved `final_csv.csv` with deduplication, per-label/per-decision caps applied.\")\n",
    "    \n",
    "    # Now produce the summary CSV:\n",
    "    generate_summary_report(final_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6fcbc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Processing file: 1.csv…\n",
      "🔄 Processing file: 1724275399_chat_logs_to_csv_0_fe6260b29bf546d984368cebd17431e0.csv…\n",
      "🔄 Processing file: 1724275399_chat_logs_to_csv_1_fe6260b29bf546d984368cebd17431e0.csv…\n",
      "🔄 Processing file: 1727392003_chat_logs_to_csv_0_cc1dcac030ce49328aa3da017ba0a963.csv…\n",
      "🔄 Processing file: 1727392003_chat_logs_to_csv_1_cc1dcac030ce49328aa3da017ba0a963.csv…\n",
      "🔄 Processing file: 1727392003_chat_logs_to_csv_2_cc1dcac030ce49328aa3da017ba0a963.csv…\n",
      "🔄 Processing file: 1727392003_chat_logs_to_csv_3_cc1dcac030ce49328aa3da017ba0a963.csv…\n",
      "🔄 Processing file: 1727392003_chat_logs_to_csv_4_cc1dcac030ce49328aa3da017ba0a963.csv…\n",
      "🔄 Processing file: 1748861002_chat_logs_to_csv_0_2bc0d3c6185d4df8af4c0abae04e85cd.csv…\n",
      "🔄 Processing file: 1748861002_chat_logs_to_csv_1_2bc0d3c6185d4df8af4c0abae04e85cd.csv…\n",
      "🔄 Processing file: 1748861002_chat_logs_to_csv_2_2bc0d3c6185d4df8af4c0abae04e85cd.csv…\n",
      "🔄 Processing file: 1748861002_chat_logs_to_csv_3_2bc0d3c6185d4df8af4c0abae04e85cd.csv…\n",
      "🔄 Processing file: 1748861002_chat_logs_to_csv_4_2bc0d3c6185d4df8af4c0abae04e85cd.csv…\n",
      "🔄 Processing file: 1748861002_chat_logs_to_csv_5_2bc0d3c6185d4df8af4c0abae04e85cd.csv…\n",
      "🔄 Processing file: 1748861002_chat_logs_to_csv_6_2bc0d3c6185d4df8af4c0abae04e85cd.csv…\n",
      "🔄 Processing file: 1749543057_chat_logs_to_csv_0_67eeaafbd8fb4ebb95129372c6ca26d1.csv…\n",
      "🔄 Processing file: 1749543057_chat_logs_to_csv_1_67eeaafbd8fb4ebb95129372c6ca26d1.csv…\n",
      "🔄 Processing file: 1749543057_chat_logs_to_csv_2_67eeaafbd8fb4ebb95129372c6ca26d1.csv…\n",
      "🔄 Processing file: 1749543057_chat_logs_to_csv_3_67eeaafbd8fb4ebb95129372c6ca26d1.csv…\n",
      "🔄 Processing file: 1749543057_chat_logs_to_csv_4_67eeaafbd8fb4ebb95129372c6ca26d1.csv…\n",
      "🔄 Processing file: 1749714117.csv…\n",
      "🔄 Processing file: 1749714117_chat_logs_to_csv_0_89306852082b465eb7361c5a715b3eae.csv…\n",
      "🔄 Processing file: 1749714117_chat_logs_to_csv_1_89306852082b465eb7361c5a715b3eae.csv…\n",
      "🔄 Processing file: 1749714117_chat_logs_to_csv_2_89306852082b465eb7361c5a715b3eae.csv…\n",
      "🔄 Processing file: 2.csv…\n",
      "🔄 Processing file: csv0.csv…\n",
      "🔄 Processing file: csv1.csv…\n",
      "✅ Saved `final_csv.csv` with deduplication, per-label/per-decision caps applied.\n",
      "\n",
      "📊 Summary Report (Totals Removed):\n",
      "\n",
      "Overall accepted : 57405\n",
      "Overall pending  : 56933\n",
      "Overall blocked  : 56024\n",
      "\n",
      "Label                         Acc     Pen     Blk\n",
      "Bullying                     5000    5000    5000\n",
      "Fighting                     5000    5000    5000\n",
      "Sexting                      5000    5000    5000\n",
      "Vulgar                        747    5000    5000\n",
      "Drugs                        2668    5000    5000\n",
      "InGame                       5000      37       0\n",
      "Alarm                          84    4555    5000\n",
      "Fraud                        3209    4451    5000\n",
      "Racist                       1591    2429    5000\n",
      "Religion                     5000    5000    5000\n",
      "Junk                         5000    3815       0\n",
      "Website                      4503    5000    5000\n",
      "Grooming                     3474    5000    5000\n",
      "Politics                     3629    1646    1024\n",
      "Nothing Wrong                7500       0       0\n"
     ]
    }
   ],
   "source": [
    "# ──────────────── STEP 5: Run on your folder ────────────────\n",
    "folder_path = r\"U:\\N\\csv\"\n",
    "file_clean_summary_for_folder(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88c096a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training records written: 153325\n",
      "Validation records written: 17037\n",
      "Validation passed: gemini_ft_train.jsonl is Gemini-compatible JSONL.\n",
      "Validation passed: gemini_ft_val.jsonl is Gemini-compatible JSONL.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import sys\n",
    "import random\n",
    "\n",
    "# Fixed list of moderation categories in desired order\n",
    "CATEGORIES = [\n",
    "    \"Junk\", \"Vulgar\", \"Nothing Wrong\", \"Bullying\",\n",
    "    \"Fighting\", \"Sexting\", \"Drug References\", \"In-Game Content\",\n",
    "    \"Fraud\", \"Racism\", \"Sentiment\", \"Grooming Attempts\",\n",
    "    \"RealName\", \"Website\", \"Alarm\", \"Politics\", \"Religion\", \"Subversive\"\n",
    "]\n",
    "\n",
    "# System instruction with added severity interpretation\n",
    "SYSTEM_TEXT = (\n",
    "    \"You are analyzing a message from the Animal Jam chat system to determine \"\n",
    "    \"if it contains any inappropriate or harmful content that may violate community guidelines.\\n\\n\"\n",
    "    \"For each relevant category (Bullying, Fighting, Sexting, Vulgar Language, Drug References, \"\n",
    "    \"In-Game Content, Fraud, Racism, Sentiment, Grooming Attempts), assign a severity score 0–7. \"\n",
    "    \"Output ONLY the JSON object with all categories (zeros included) in this fixed key order.\\n\\n\"\n",
    "    \"Severity interpretation:\\n\"\n",
    "    \"  - 0 to 2: Safe for kids\\n\"\n",
    "    \"  - 3 to 4: Might be bad but okish\\n\"\n",
    "    \"  - 5 to 7: Bad and should not be read by kids\"\n",
    ")\n",
    "\n",
    "input_file = r'U:\\N\\final_csv.csv'\n",
    "train_output_file = 'gemini_ft_train.jsonl'\n",
    "val_output_file = 'gemini_ft_val.jsonl'\n",
    "VAL_RATIO = 0.1  # 10% of data will go to validation\n",
    "\n",
    "def build_record(text, label, severity):\n",
    "    scores = {cat: 0 for cat in CATEGORIES}\n",
    "    if label in scores:\n",
    "        scores[label] = severity\n",
    "\n",
    "    return {\n",
    "        \"systemInstruction\": {\n",
    "            \"role\": \"system\",\n",
    "            \"parts\": [{\"text\": SYSTEM_TEXT}]\n",
    "        },\n",
    "        \"contents\": [\n",
    "            {\"role\": \"user\", \"parts\": [{\"text\": text}]},\n",
    "            {\"role\": \"model\", \"parts\": [{\"text\": json.dumps(scores, separators=(',', ':'))}]}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def convert_and_split_csv():\n",
    "    with open(input_file, newline='', encoding='utf-8') as csvf:\n",
    "        reader = list(csv.DictReader(csvf))\n",
    "        random.shuffle(reader)\n",
    "\n",
    "        split_idx = int(len(reader) * (1 - VAL_RATIO))\n",
    "        train_rows = reader[:split_idx]\n",
    "        val_rows = reader[split_idx:]\n",
    "\n",
    "        train_count = 0\n",
    "        val_count = 0\n",
    "\n",
    "        with open(train_output_file, 'w', encoding='utf-8') as train_outf, \\\n",
    "             open(val_output_file, 'w', encoding='utf-8') as val_outf:\n",
    "\n",
    "            for row in train_rows:\n",
    "                text = row.get('\\ufeffText', row.get('Text', '')).strip()\n",
    "                label = row.get('Label', '').strip()\n",
    "                raw = row.get('Status', '0').strip()\n",
    "                try:\n",
    "                    severity = int(float(raw))\n",
    "                except ValueError:\n",
    "                    severity = 0\n",
    "\n",
    "                record = build_record(text, label, severity)\n",
    "                train_outf.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "                train_count += 1\n",
    "\n",
    "            for row in val_rows:\n",
    "                text = row.get('\\ufeffText', row.get('Text', '')).strip()\n",
    "                label = row.get('Label', '').strip()\n",
    "                raw = row.get('Status', '0').strip()\n",
    "                try:\n",
    "                    severity = int(float(raw))\n",
    "                except ValueError:\n",
    "                    severity = 0\n",
    "\n",
    "                record = build_record(text, label, severity)\n",
    "                val_outf.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "                val_count += 1\n",
    "\n",
    "    print(f\"Training records written: {train_count}\")\n",
    "    print(f\"Validation records written: {val_count}\")\n",
    "\n",
    "\n",
    "def validate_first_record(path):\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        line = f.readline()\n",
    "    try:\n",
    "        rec = json.loads(line)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON parse error in {path}: {e}\")\n",
    "        sys.exit(1)\n",
    "    if 'systemInstruction' not in rec or 'contents' not in rec:\n",
    "        print(f\"Format error in {path}: Missing 'systemInstruction' or 'contents'\")\n",
    "        sys.exit(1)\n",
    "    if not isinstance(rec['contents'], list) or len(rec['contents']) != 2:\n",
    "        print(f\"Format error in {path}: 'contents' must be a list of two entries (user, model)\")\n",
    "        sys.exit(1)\n",
    "    print(f\"Validation passed: {path} is Gemini-compatible JSONL.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    convert_and_split_csv()\n",
    "    validate_first_record(train_output_file)\n",
    "    validate_first_record(val_output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "681de9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# import json\n",
    "# import sys\n",
    "\n",
    "# # Fixed list of moderation categories in desired order\n",
    "# CATEGORIES = [\n",
    "#     \"Junk\", \"Vulgar\", \"Nothing Wrong\", \"Bullying\",\n",
    "#     \"Fighting\", \"Sexting\", \"Drug References\", \"In-Game Content\",\n",
    "#     \"Fraud\", \"Racism\", \"Sentiment\", \"Grooming Attempts\",\n",
    "#     \"RealName\", \"Website\", \"Alarm\", \"Politics\", \"Religion\", \"Subversive\"\n",
    "# ]\n",
    "\n",
    "# # System instruction with added severity interpretation\n",
    "# SYSTEM_TEXT = (\n",
    "#     \"You are analyzing a message from the Animal Jam chat system to determine \"\n",
    "#     \"if it contains any inappropriate or harmful content that may violate community guidelines.\\n\\n\"\n",
    "#     \"For each relevant category (Bullying, Fighting, Sexting, Vulgar Language, Drug References, \"\n",
    "#     \"In-Game Content, Fraud, Racism, Sentiment, Grooming Attempts), assign a severity score 0–7. \"\n",
    "#     \"Output ONLY the JSON object with all categories (zeros included) in this fixed key order.\\n\\n\"\n",
    "#     \"Severity interpretation:\\n\"\n",
    "#     \"  - 0 to 2: Safe for kids\\n\"\n",
    "#     \"  - 3 to 4: Might be bad but okish\\n\"\n",
    "#     \"  - 5 to 7: Bad and should not be read by kids\"\n",
    "# )\n",
    "\n",
    "# input_file = r'U:\\N\\final_csv.csv'\n",
    "# output_file = 'gemini_ft_dataset.jsonl'\n",
    "\n",
    "\n",
    "# def convert_csv_to_gemini_jsonl():\n",
    "#     with open(input_file, newline='', encoding='utf-8') as csvf, \\\n",
    "#          open(output_file, 'w', encoding='utf-8') as outf:\n",
    "\n",
    "#         reader = csv.DictReader(csvf)\n",
    "#         for row in reader:\n",
    "#             # Extract text and label\n",
    "#             text = row.get('\\ufeffText', row.get('Text', '')).strip()\n",
    "#             label = row.get('Label', '').strip()\n",
    "#             raw = row.get('Status', '0').strip()\n",
    "#             try:\n",
    "#                 severity = int(float(raw))\n",
    "#             except ValueError:\n",
    "#                 severity = 0\n",
    "\n",
    "#             # Populate all categories with zero, then set the one label\n",
    "#             scores = {cat: 0 for cat in CATEGORIES}\n",
    "#             if label in scores:\n",
    "#                 scores[label] = severity\n",
    "\n",
    "#             # Build the Gemini-style record\n",
    "#             record = {\n",
    "#                 \"systemInstruction\": {\n",
    "#                     \"role\": \"system\",\n",
    "#                     \"parts\": [{\"text\": SYSTEM_TEXT}]\n",
    "#                 },\n",
    "#                 \"contents\": [\n",
    "#                     {\n",
    "#                         \"role\": \"user\",\n",
    "#                         \"parts\": [{\"text\": text}]\n",
    "#                     },\n",
    "#                     {\n",
    "#                         \"role\": \"model\",\n",
    "#                         \"parts\": [{\"text\": json.dumps(scores, separators=(',', ':'))}]\n",
    "#                     }\n",
    "#                 ]\n",
    "#             }\n",
    "#             outf.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "# def validate_first_record():\n",
    "#     # Quick check of structure\n",
    "#     with open(output_file, encoding='utf-8') as f:\n",
    "#         line = f.readline()\n",
    "#     try:\n",
    "#         rec = json.loads(line)\n",
    "#     except json.JSONDecodeError as e:\n",
    "#         print(f\"JSON parse error: {e}\")\n",
    "#         sys.exit(1)\n",
    "#     # Check required fields\n",
    "#     if 'systemInstruction' not in rec or 'contents' not in rec:\n",
    "#         print(\"Format error: Missing 'systemInstruction' or 'contents' field\")\n",
    "#         sys.exit(1)\n",
    "#     if not isinstance(rec['contents'], list) or len(rec['contents']) != 2:\n",
    "#         print(\"Format error: 'contents' must be a list of two entries (user, model)\")\n",
    "#         sys.exit(1)\n",
    "#     print(\"Validation passed: Dataset is Gemini-compatible JSONL.\")\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     convert_csv_to_gemini_jsonl()\n",
    "#     validate_first_record()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7570132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# import json\n",
    "# from collections import OrderedDict\n",
    "\n",
    "# # Your fixed list of categories, in the exact order you want them in the output JSON\n",
    "# CATEGORIES = [\n",
    "#     \"Junk\", \"Vulgar\", \"Nothing Wrong\", \"Bullying\",\n",
    "#     \"Fighting\", \"Sexting\", \"Drug References\", \"In-Game Content\",\n",
    "#     \"Fraud\", \"Racism\", \"Sentiment\", \"Grooming Attempts\",\n",
    "#     \"RealName\", \"Website\", \"Alarm\", \"Politics\", \"Religion\", \"Subversive\"\n",
    "# ]\n",
    "\n",
    "# # System instruction with added severity interpretation\n",
    "# SYSTEM_TEXT = (\n",
    "#     \"You are analyzing a message from the Animal Jam chat system to determine \"\n",
    "#     \"if it contains any inappropriate or harmful content that may violate community guidelines.\\n\\n\"\n",
    "#     \"For each relevant category (Bullying, Fighting, Sexting, Vulgar Language, Drug References, \"\n",
    "#     \"In-Game Content, Fraud, Racism, Sentiment, Grooming Attempts), assign a severity score 0–7. \"\n",
    "#     \"Output ONLY the JSON object with all categories (zeros included) in this fixed key order.\\n\\n\"\n",
    "#     \"Severity interpretation:\\n\"\n",
    "#     \"  - 0 to 2: Safe for kids\\n\"\n",
    "#     \"  - 3 to 4: Might be bad but okish\\n\"\n",
    "#     \"  - 5 to 7: Bad and should not be read by kids\"\n",
    "# )\n",
    "\n",
    "# input_file = r'U:\\N\\final_csv.csv'\n",
    "# output_file = 'ft_input.jsonl'\n",
    "\n",
    "# with open(input_file, newline='', encoding='utf-8') as csvfile, \\\n",
    "#      open(output_file, 'w', encoding='utf-8') as jsonlfile:\n",
    "\n",
    "#     reader = csv.DictReader(csvfile)\n",
    "#     for row in reader:\n",
    "#         # grab the text field (handle BOM if present)\n",
    "#         text = row.get('\\ufeffText', row.get('Text', '')).strip()\n",
    "#         label = row.get('Label', '').strip()\n",
    "#         raw_status = row.get('Status', '0').strip()\n",
    "#         try:\n",
    "#             severity = int(float(raw_status))\n",
    "#         except ValueError:\n",
    "#             severity = 0\n",
    "\n",
    "#         # Build a full dict of all categories, zeros by default\n",
    "#         scores = OrderedDict((cat, 0) for cat in CATEGORIES)\n",
    "#         # only overwrite the one category this row labels\n",
    "#         if label in scores:\n",
    "#             scores[label] = severity\n",
    "\n",
    "#         # Serialize the JSON object with no additional whitespace\n",
    "#         completion_str = json.dumps(scores, separators=(',', ':'))\n",
    "\n",
    "#         # Build the prompt: system + user + assistant marker\n",
    "#         prompt = (\n",
    "#             f\"<|system|> {SYSTEM_TEXT}\\n\"\n",
    "#             f\"<|user|> {text}\\n\"\n",
    "#             f\"<|assistant|>\"\n",
    "#         )\n",
    "\n",
    "#         # Vertex AI wants a leading space before the completion, and a trailing newline\n",
    "#         record = {\n",
    "#             \"prompt\": prompt,\n",
    "#             \"completion\": \" \" + completion_str + \"\\n\"\n",
    "#         }\n",
    "\n",
    "#         jsonlfile.write(json.dumps(record) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69480d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Load your CSV (adjust path as needed)\n",
    "# df = pd.read_csv(\"download.csv\")\n",
    "\n",
    "# # Replace \"locked\" with \"blocked\"\n",
    "# df['Decision'] = df['Decision'].replace('locked', 'blocked')\n",
    "\n",
    "# # Save to desired drive and folder\n",
    "# df.to_csv(r\"U:\\N\\updated_decision_column.csv\", index=False)\n",
    "# print(\"File saved to U:\\\\N\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "129cee31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Paths to your input files\n",
    "# file1 = 'final_csv.csv'\n",
    "# file2 = 'updated_decision_column.csv'\n",
    "\n",
    "# # Read them into DataFrames\n",
    "# df1 = pd.read_csv(file1)\n",
    "# df2 = pd.read_csv(file2)\n",
    "\n",
    "# # Concatenate vertically (one below the other)\n",
    "# combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# # Optionally: drop duplicate rows, if needed\n",
    "# # combined_df = combined_df.drop_duplicates()\n",
    "\n",
    "# # Write out to a new CSV\n",
    "# output_file = 'combined_output.csv'\n",
    "# combined_df.to_csv(output_file, index=False)\n",
    "\n",
    "# print(f\"Combined CSV saved to: {output_file}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
