{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f933b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utkarsh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 698274/698274 [43:10<00:00, 269.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Metrics:\n",
      "Label Accuracy: 0.9269\n",
      "Status Accuracy: 0.9193\n",
      "Decision Accuracy: 0.9247\n",
      "Errors: 0/698274 samples\n",
      "Saved label_confusion_matrix.png\n",
      "Saved status_confusion_matrix.png\n",
      "Saved decision_confusion_matrix.png\n",
      "Saved label_classification_report.txt\n",
      "Saved status_classification_report.txt\n",
      "Saved decision_classification_report.txt\n",
      "\n",
      "Saved full_report.txt with comprehensive evaluation metrics\n",
      "\n",
      "Saved predictions to 'predictions_report.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import DistilBertTokenizerFast\n",
    "import joblib\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from transformers import DistilBertModel, DistilBertTokenizerFast, get_linear_schedule_with_warmup\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import DistilBertTokenizerFast\n",
    "import joblib\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Suppress warnings if needed\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load model and resources\n",
    "model_dir = r\"U:\\N\\save\"\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_dir)\n",
    "label_enc = joblib.load(os.path.join(model_dir, \"label_encoder.pkl\"))\n",
    "status_enc = joblib.load(os.path.join(model_dir, \"status_encoder.pkl\"))\n",
    "status_map = {0: 'accepted', 1: 'accepted', 2: 'accepted',\n",
    "              3: 'pending', 4: 'pending',\n",
    "              5: 'blocked', 6: 'blocked', 7: 'blocked'}\n",
    "\n",
    "# Model definition (must match training architecture)\n",
    "class ChatModerationModel(torch.nn.Module):\n",
    "    def __init__(self, num_labels, num_statuses):\n",
    "        super().__init__()\n",
    "        self.bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        for layer in self.bert.transformer.layer[:3]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False\n",
    "        self.dropout = torch.nn.Dropout(0.6)\n",
    "        self.label_head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.bert.config.hidden_size, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.4),\n",
    "            torch.nn.Linear(256, num_labels)\n",
    "        )\n",
    "        self.status_head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.bert.config.hidden_size, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.4),\n",
    "            torch.nn.Linear(256, num_statuses)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = self.dropout(out.last_hidden_state[:, 0])\n",
    "        return self.label_head(pooled), self.status_head(pooled)\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ChatModerationModel(\n",
    "    num_labels=len(label_enc.classes_),\n",
    "    num_statuses=len(status_enc.classes_)\n",
    ")\n",
    "model.load_state_dict(torch.load(os.path.join(model_dir, \"model_state.pt\"), map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Prediction function\n",
    "def predict(text):\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits_label, logits_status = model(**inputs)\n",
    "    \n",
    "    # Get predictions\n",
    "    lbl_idx = logits_label.argmax(dim=1).item()\n",
    "    stt_idx = logits_status.argmax(dim=1).item()\n",
    "    \n",
    "    predicted_label = label_enc.inverse_transform([lbl_idx])[0]\n",
    "    predicted_status = status_enc.inverse_transform([stt_idx])[0]\n",
    "    predicted_decision = status_map[stt_idx]\n",
    "    \n",
    "    return predicted_label, predicted_status, predicted_decision\n",
    "\n",
    "# Process CSV with proper encoding\n",
    "try:\n",
    "    # Try UTF-8 first\n",
    "    df = pd.read_csv(\"Test_csv.csv\", encoding='utf-8')\n",
    "except UnicodeDecodeError:\n",
    "    try:\n",
    "        # Fall back to UTF-16 if UTF-8 fails\n",
    "        df = pd.read_csv(\"Test_csv.csv\", encoding='utf-16')\n",
    "    except UnicodeDecodeError:\n",
    "        # Try other common encodings if needed\n",
    "        df = pd.read_csv(\"Test_csv.csv\", encoding='latin1')\n",
    "\n",
    "tqdm.pandas(desc=\"Processing predictions\")\n",
    "\n",
    "# Run predictions and store results\n",
    "results = []\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    text = row['Text']\n",
    "    try:\n",
    "        pred_label, pred_status, pred_decision = predict(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing: {text[:50]}... - {str(e)}\")\n",
    "        pred_label, pred_status, pred_decision = \"ERROR\", \"ERROR\", \"ERROR\"\n",
    "    \n",
    "    results.append({\n",
    "        \"Original Text\": text,\n",
    "        \"Original Label\": row[\"Label\"],\n",
    "        \"Original Status\": row[\"Status\"],\n",
    "        \"Original Decision\": row[\"Decision\"],\n",
    "        \"Predicted Label\": pred_label,\n",
    "        \"Predicted Status\": pred_status,\n",
    "        \"Predicted Decision\": pred_decision\n",
    "    })\n",
    "\n",
    "# Create results dataframe\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Filter out errors for metrics\n",
    "error_mask = (results_df[\"Predicted Label\"] == \"ERROR\") | (results_df[\"Predicted Status\"] == \"ERROR\")\n",
    "num_errors = error_mask.sum()\n",
    "results_df_clean = results_df[~error_mask]\n",
    "\n",
    "# Calculate accuracy\n",
    "label_acc = np.mean(results_df_clean[\"Original Label\"] == results_df_clean[\"Predicted Label\"])\n",
    "status_acc = np.mean(results_df_clean[\"Original Status\"].astype(str) == results_df_clean[\"Predicted Status\"].astype(str))\n",
    "decision_acc = np.mean(results_df_clean[\"Original Decision\"] == results_df_clean[\"Predicted Decision\"])\n",
    "\n",
    "print(f\"\\nEvaluation Metrics:\")\n",
    "print(f\"Label Accuracy: {label_acc:.4f}\")\n",
    "print(f\"Status Accuracy: {status_acc:.4f}\")\n",
    "print(f\"Decision Accuracy: {decision_acc:.4f}\")\n",
    "print(f\"Errors: {num_errors}/{len(df)} samples\")\n",
    "\n",
    "# ========================\n",
    "# Enhanced Reporting\n",
    "# ========================\n",
    "\n",
    "# 1. Confusion Matrices\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, title, filename, figsize=(12, 10)):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=classes, yticklabels=classes)\n",
    "    plt.title(f'Confusion Matrix: {title}', fontsize=14)\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved {filename}\")\n",
    "\n",
    "# Get unique classes present in data\n",
    "label_classes = sorted(set(results_df_clean[\"Original Label\"].unique()).union(\n",
    "                     set(results_df_clean[\"Predicted Label\"].unique())))\n",
    "status_classes = sorted(set(results_df_clean[\"Original Status\"].astype(str).unique()).union(\n",
    "                     set(results_df_clean[\"Predicted Status\"].astype(str).unique())))\n",
    "decision_classes = ['accepted', 'pending', 'blocked']\n",
    "\n",
    "# Generate confusion matrices\n",
    "plot_confusion_matrix(\n",
    "    results_df_clean[\"Original Label\"],\n",
    "    results_df_clean[\"Predicted Label\"],\n",
    "    label_classes,\n",
    "    \"Language Labels\",\n",
    "    \"label_confusion_matrix.png\"\n",
    ")\n",
    "\n",
    "plot_confusion_matrix(\n",
    "    results_df_clean[\"Original Status\"].astype(str),\n",
    "    results_df_clean[\"Predicted Status\"].astype(str),\n",
    "    status_classes,\n",
    "    \"Status Labels\",\n",
    "    \"status_confusion_matrix.png\",\n",
    "    figsize=(14, 12)\n",
    ")\n",
    "\n",
    "plot_confusion_matrix(\n",
    "    results_df_clean[\"Original Decision\"],\n",
    "    results_df_clean[\"Predicted Decision\"],\n",
    "    decision_classes,\n",
    "    \"Decision Labels\",\n",
    "    \"decision_confusion_matrix.png\"\n",
    ")\n",
    "\n",
    "# 2. Classification Reports\n",
    "def save_classification_report(y_true, y_pred, classes, title, filename):\n",
    "    report = classification_report(\n",
    "        y_true, \n",
    "        y_pred, \n",
    "        labels=classes,\n",
    "        target_names=classes, \n",
    "        digits=4,\n",
    "        zero_division=0\n",
    "    )\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(f\"Classification Report: {title}\\n\")\n",
    "        f.write(\"===================================\\n\\n\")\n",
    "        f.write(report)\n",
    "    print(f\"Saved {filename}\")\n",
    "    return report\n",
    "\n",
    "# Save reports\n",
    "label_report = save_classification_report(\n",
    "    results_df_clean[\"Original Label\"],\n",
    "    results_df_clean[\"Predicted Label\"],\n",
    "    label_classes,\n",
    "    \"Language Labels\",\n",
    "    \"label_classification_report.txt\"\n",
    ")\n",
    "\n",
    "status_report = save_classification_report(\n",
    "    results_df_clean[\"Original Status\"].astype(str),\n",
    "    results_df_clean[\"Predicted Status\"].astype(str),\n",
    "    status_classes,\n",
    "    \"Status Labels\",\n",
    "    \"status_classification_report.txt\"\n",
    ")\n",
    "\n",
    "decision_report = save_classification_report(\n",
    "    results_df_clean[\"Original Decision\"],\n",
    "    results_df_clean[\"Predicted Decision\"],\n",
    "    decision_classes,\n",
    "    \"Decision Labels\",\n",
    "    \"decision_classification_report.txt\"\n",
    ")\n",
    "\n",
    "# 3. Error Analysis\n",
    "error_df = results_df[error_mask]\n",
    "if not error_df.empty:\n",
    "    error_df.to_csv(\"prediction_errors.csv\", index=False, encoding='utf-8-sig')\n",
    "    print(f\"Saved error samples to prediction_errors.csv ({len(error_df)} errors)\")\n",
    "\n",
    "# 4. Class-wise Accuracy\n",
    "def calculate_class_accuracy(df, class_col, pred_col):\n",
    "    class_acc = {}\n",
    "    for cls in df[class_col].unique():\n",
    "        cls_mask = df[class_col] == cls\n",
    "        if cls_mask.sum() > 0:\n",
    "            acc = np.mean(df.loc[cls_mask, class_col] == df.loc[cls_mask, pred_col])\n",
    "            class_acc[cls] = acc\n",
    "    return class_acc\n",
    "\n",
    "label_class_acc = calculate_class_accuracy(results_df_clean, \"Original Label\", \"Predicted Label\")\n",
    "status_class_acc = calculate_class_accuracy(results_df_clean, \"Original Status\", \"Predicted Status\")\n",
    "decision_class_acc = calculate_class_accuracy(results_df_clean, \"Original Decision\", \"Predicted Decision\")\n",
    "\n",
    "# 5. Save comprehensive report\n",
    "with open(\"full_report.txt\", \"w\", encoding='utf-8') as f:\n",
    "    f.write(\"Chat Moderation Model Evaluation Report\\n\")\n",
    "    f.write(\"======================================\\n\\n\")\n",
    "    f.write(f\"Total Samples: {len(df)}\\n\")\n",
    "    f.write(f\"Errors: {num_errors}\\n\")\n",
    "    f.write(f\"Label Accuracy: {label_acc:.4f}\\n\")\n",
    "    f.write(f\"Status Accuracy: {status_acc:.4f}\\n\")\n",
    "    f.write(f\"Decision Accuracy: {decision_acc:.4f}\\n\\n\")\n",
    "    \n",
    "    f.write(\"\\nLabel Class Accuracy:\\n\")\n",
    "    for cls, acc in label_class_acc.items():\n",
    "        f.write(f\"  {cls}: {acc:.4f}\\n\")\n",
    "    \n",
    "    f.write(\"\\nStatus Class Accuracy:\\n\")\n",
    "    for cls, acc in status_class_acc.items():\n",
    "        f.write(f\"  {cls}: {acc:.4f}\\n\")\n",
    "    \n",
    "    f.write(\"\\nDecision Class Accuracy:\\n\")\n",
    "    for cls, acc in decision_class_acc.items():\n",
    "        f.write(f\"  {cls}: {acc:.4f}\\n\")\n",
    "    \n",
    "    f.write(\"\\n\\nLabel Classification Report:\\n\")\n",
    "    f.write(label_report)\n",
    "    \n",
    "    f.write(\"\\n\\nStatus Classification Report:\\n\")\n",
    "    f.write(status_report)\n",
    "    \n",
    "    f.write(\"\\n\\nDecision Classification Report:\\n\")\n",
    "    f.write(decision_report)\n",
    "\n",
    "print(\"\\nSaved full_report.txt with comprehensive evaluation metrics\")\n",
    "\n",
    "# Save to CSV with UTF-8 encoding and proper escaping\n",
    "results_df.to_csv(\"predictions_report.csv\", index=False, encoding='utf-8-sig', escapechar='\\\\', quoting=1)\n",
    "print(\"\\nSaved predictions to 'predictions_report.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3867f43d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
